<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>L1 과 L2 정규화 (Regularization) | Ganni Devlog</title>
<meta name="keywords" content="오버피팅, Regularization, 일반화" />
<meta name="description" content="모델의 성능을 높이는 데에는 여러 방법이 있습니다. Overfitting(과대적합)을 억제하는 방법 중 L1 정규화 및 L2 정규화 기술에 대해 알아보겠습니다.
 Overfitting 아래 그림의 맨 오른쪽은 모델이 주어진 데이터 세트에 대해 지나치게 많이 학습을 한 Overfitting(과대적합)의 모습을 보여줍니다. 이는 모델이 학습 데이터를 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있습니다.
학습 데이터에 모델이 Overfitting 되는 현상은 모델의 성능을 떨어트리는 주요 이슈입니다. 훈련 데이터에 대한 정확도는 높을지라도 새로운 데이터, 즉 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않습니다.">
<meta name="author" content="">
<link rel="canonical" href="https://koolganni.github.io/posts/machine-learning/l1-l2-regularization/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.b15591da31b4f827d6dc0b97cf931d21b01610305321ac220bf755cf6aade94e.css" integrity="sha256-sVWR2jG0&#43;CfW3AuXz5MdIbAWEDBTIawiC/dVz2qt6U4=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://koolganni.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://koolganni.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://koolganni.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://koolganni.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://koolganni.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.81.0" />
<meta property="og:title" content="L1 과 L2 정규화 (Regularization)" />
<meta property="og:description" content="모델의 성능을 높이는 데에는 여러 방법이 있습니다. Overfitting(과대적합)을 억제하는 방법 중 L1 정규화 및 L2 정규화 기술에 대해 알아보겠습니다.
 Overfitting 아래 그림의 맨 오른쪽은 모델이 주어진 데이터 세트에 대해 지나치게 많이 학습을 한 Overfitting(과대적합)의 모습을 보여줍니다. 이는 모델이 학습 데이터를 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있습니다.
학습 데이터에 모델이 Overfitting 되는 현상은 모델의 성능을 떨어트리는 주요 이슈입니다. 훈련 데이터에 대한 정확도는 높을지라도 새로운 데이터, 즉 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않습니다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://koolganni.github.io/posts/machine-learning/l1-l2-regularization/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-06T20:21:54&#43;09:00" />
<meta property="article:modified_time" content="2021-02-06T20:21:54&#43;09:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="L1 과 L2 정규화 (Regularization)"/>
<meta name="twitter:description" content="모델의 성능을 높이는 데에는 여러 방법이 있습니다. Overfitting(과대적합)을 억제하는 방법 중 L1 정규화 및 L2 정규화 기술에 대해 알아보겠습니다.
 Overfitting 아래 그림의 맨 오른쪽은 모델이 주어진 데이터 세트에 대해 지나치게 많이 학습을 한 Overfitting(과대적합)의 모습을 보여줍니다. 이는 모델이 학습 데이터를 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있습니다.
학습 데이터에 모델이 Overfitting 되는 현상은 모델의 성능을 떨어트리는 주요 이슈입니다. 훈련 데이터에 대한 정확도는 높을지라도 새로운 데이터, 즉 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않습니다."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://koolganni.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "L1 과 L2 정규화 (Regularization)",
      "item": "https://koolganni.github.io/posts/machine-learning/l1-l2-regularization/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "L1 과 L2 정규화 (Regularization)",
  "name": "L1 과 L2 정규화 (Regularization)",
  "description": "모델의 성능을 높이는 데에는 여러 방법이 있습니다. Overfitting(과대적합)을 억제하는 방법 중 L1 정규화 및 L2 정규화 기술에 대해 알아보겠습니다.\n Overfitting 아래 그림의 맨 오른쪽은 모델이 주어진 데이터 세트에 대해 지나치게 많이 학습을 한 Overfitting(과대적합)의 모습을 보여줍니다. 이는 모델이 학습 데이터를 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있습니다.\n학습 데이터에 모델이 Overfitting 되는 현상은 모델의 성능을 떨어트리는 주요 이슈입니다. 훈련 데이터에 대한 정확도는 높을지라도 새로운 데이터, 즉 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않습니다.",
  "keywords": [
    "오버피팅", "Regularization", "일반화"
  ],
  "articleBody": " 모델의 성능을 높이는 데에는 여러 방법이 있습니다. Overfitting(과대적합)을 억제하는 방법 중 L1 정규화 및 L2 정규화 기술에 대해 알아보겠습니다.\n Overfitting 아래 그림의 맨 오른쪽은 모델이 주어진 데이터 세트에 대해 지나치게 많이 학습을 한 Overfitting(과대적합)의 모습을 보여줍니다. 이는 모델이 학습 데이터를 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있습니다.\n학습 데이터에 모델이 Overfitting 되는 현상은 모델의 성능을 떨어트리는 주요 이슈입니다. 훈련 데이터에 대한 정확도는 높을지라도 새로운 데이터, 즉 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않습니다.\nOverfitting은 주로 매개변수가 많고 표현력이 높은 복잡한 모델이거나 훈련 데이터가 적은 경우에 발생합니다. 따라서 이를 방지하기 위한 방법에는 다음과 같은 방법들이 있습니다.\n 더 많은 데이터를 활용한다. Feature 의 개수를 줄인다. 적절한 파라미터를 선정한다. Regularization (정규화)  이 중 Regularization은 Overfitting 억제를 위해 많이 사용하는 방법으로 학습 과정에서 큰 가중치에 대해서는 그에 상응하는 큰 패널티를 부과하는 방식으로 진행됩니다. 가중치 매개변수의 값이 커서 발생하는 과대적합을 패널티에 의한 가중치 감소(Weight Decay) 를 통해 억제하는 것입니다.\n이러한 가중치에 대한 패널티를 어떤 방식으로 주는지에 따라 L1 Regularization (Lasso), L2 Regularization (Ridge) 크게 두 가지의 방법이 존재합니다.\n L1 Regularization (Lasso) Regularization은 모델을 학습할 때 단순히 손실 뿐만이 아니라 모델의 복잡도를 함께 최소화해 모델이 일반화가 될 수 있도록 합니다. 정규화를 적용한 모델의 학습 최적화 알고리즘은 모델이 데이터에 얼마나 적합한지 측정하는 손실 항과 모델 복잡도를 측정하는 정규화 항의 함수가 됩니다.\n이때 L1 Regularization은 정규화 항을 L1 Norm1 을 기반으로 정의합니다. L1 Norm은 Manhattan distance 로 부르기도 하는데 벡터 a, b 가 있을 때 각 원소들의 차이의 절댓값 총합을 의미합니다.\n따라서 L1 정규화 항은 모든 특성 가중치의 절댓값의 합계로 정의해 모델의 복잡도를 수치화합니다. 아래 식은 기존 비용 함수에 L1 정규화 항을 더한 것입니다.\n ƛ(lambda) : Regularization 의 강도를 결정하는 하이퍼파라미터  L1 정규화를 적용한 비용 함수는 가중치 값에 대한 미분을 통해 가중치에서 일정 상수를 빼는 식으로 가중치를 업데이트합니다.\n최종적으로 모델에서 유용하지 않은 일부 가중치를 0이 되도록 유도할 수 있습니다. 가중치가 정확하게 0일 경우 이것은 모델에서 해당 특성을 삭제하는 것과 같습니다.\n L1 Regularization을 사용하는 Regression 모델을 Lasso Regression 이라고 합니다.   L2 Regularization (Ridge) L2 Regularization은 정규화 항을 L2 Norm을 기반으로 정의합니다. L2 Norm은 Euclidean distance 로 부르기도 하는데 벡터 a, b 가 있을 때 두 벡터의 직선 거리를 의미합니다.\n따라서 L2 정규화 항은 모든 특성 가중치를 제곱한 값의 합계로 정의해 모델의 복잡도를 수치화합니다. 아래 식은 기존 비용 함수에 L2 정규화 항을 더한 것입니다.\nL2 정규화를 적용한 비용 함수는 가중치 값에 대한 미분을 통해 아래와 같은 식으로 가중치를 업데이트합니다. 이때 기존 가중치(θj)에 곱해지는 값은 항상 1 보다 작으므로 최종적으로 가중치는 줄어드는 방향으로 조정되고 특정 가중치가 비이상적으로 커지는 것을 막습니다.\n📌 ƛ(lambda) 값이 너무 높으면 모델은 단순해지지만 일반화가 아니라 Underfitting(과소적합)해질 위험이 있습니다. 반대로 값이 너무 낮으면 모델은 더 복잡해지고 Overfitting(과대적합)해질 위험이 있습니다. 0으로 설정하는 경우 정규화는 완전히 제거되어 가장 높은 수준의 과적합 위험을 갖게 됩니다. 이것은 L1 Regularization 에서도 마찬가지입니다.\n L2 Regularization을 사용하는 Regression 모델을 Ridge Regression 이라고 합니다.   L1 Regularization 🆚 L2 Regularization L1 정규화와 L2 정규화의 가중치 업데이트에 대한 차이점을 좀 더 살펴보겠습니다.\n아래 그림의 초록색 부분은 정규화로 가능한 가중치의 범위를 나타내며 빨간색 부분은 비용 함수로 가운데 점은 전역 최솟값을 만드는 최적의 가중치를 의미합니다. 왼쪽 그림은 L1 정규화 오른쪽 그림은 L2 정규화에 대한 내용입니다.\nL2 정규화로 가능한 가중치의 범위는 원형의 모양으로 일반적으로 비용 함수와의 접선이 축에 위치하지 않기 때문에 가중치가 0이 되지는 않습니다.\nL1 정규화로 가능한 가중치의 범위는 뾰족한 모양으로 비용 함수와의 접선이 축에 위치할 가능성이 높기 때문에 특정 가중치가 0이 될 수 있습니다.\n정리하면 L2 정규화를 적용할 경우 가중치를 0에 가깝게 만들지만 정확이 0이 되지는 않습니다. 즉, 매번 학습을 하면서 가중치의 x% 만큼 제거한다고 생각할 수 있습니다. 때문에 특성의 가중치를 줄여서 좀 더 단순한 모델을 만듦으로써 과대적합을 억제할 수 있습니다. L2 정규화는 단순성을 위한 정규화라고도 합니다.\nL1 정규화를 적용하면 가중치를 정확히 0으로 만들 수 있기 때문에 특정 가중치를 제거할 수 있습니다. 다시 말해 일종의 Feature Selection 을 수행해서 Feature 의 개수를 줄이는 방향으로 과대적합을 억제할 수 있습니다. L1 정규화는 희소성(Sparsity)2을 위한 정규화라고도 합니다.\n References  https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization https://www.boostcourse.org/ai222/lecture/24509 밑바닥부터 시작하는 딥러닝 1 - 한빛미디어 (2017) https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036    Norm : 벡터의 길이 혹은 크기를 측정하는 방법 ↩︎\n 0이 많이 있는 자료들을 희소성이 있는 자료라 합니다. ↩︎\n   ",
  "wordCount" : "649",
  "inLanguage": "en",
  "datePublished": "2021-02-06T20:21:54+09:00",
  "dateModified": "2021-02-06T20:21:54+09:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://koolganni.github.io/posts/machine-learning/l1-l2-regularization/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ganni Devlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://koolganni.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://koolganni.github.io/" accesskey="h" title="Ganni Devlog (Alt + H)">Ganni Devlog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://koolganni.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://koolganni.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      L1 과 L2 정규화 (Regularization)
    </h1>
    <div class="post-meta">February 6, 2021
</div>
  </header> 
  <div class="post-content"><blockquote>
<p>모델의 성능을 높이는 데에는 여러 방법이 있습니다. Overfitting(과대적합)을 억제하는 방법 중 <code>L1 정규화</code> 및 <code>L2 정규화</code> 기술에 대해 알아보겠습니다.</p>
</blockquote>
<h3 id="overfitting">Overfitting<a hidden class="anchor" aria-hidden="true" href="#overfitting">#</a></h3>
<p>아래 그림의 맨 오른쪽은 모델이 주어진 데이터 세트에 대해 지나치게 많이 학습을 한 <code>Overfitting(과대적합)</code>의 모습을 보여줍니다. 이는 모델이 학습 데이터를 과하게 <strong>암기</strong>하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있습니다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/118359458-8bdf1a00-b5be-11eb-99b3-3a6f231a7628.png" alt="image"  />
</p>
<p>학습 데이터에 모델이 <code>Overfitting</code> 되는 현상은 모델의 성능을 떨어트리는 주요 이슈입니다. 훈련 데이터에 대한 정확도는 높을지라도 <strong>새로운 데이터,</strong> 즉 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않습니다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/118364303-e8e4cb00-b5d2-11eb-926a-d89695bb5d39.png" alt="image"  />
</p>
<p><code>Overfitting</code>은 주로 매개변수가 많고 표현력이 높은 복잡한 모델이거나 훈련 데이터가 적은 경우에 발생합니다. 따라서 이를 방지하기 위한 방법에는 다음과 같은 방법들이 있습니다.</p>
<ul>
<li>더 많은 데이터를 활용한다.</li>
<li>Feature 의 개수를 줄인다.</li>
<li>적절한 파라미터를 선정한다.</li>
<li>Regularization (정규화)</li>
</ul>
<p>이 중 <code>Regularization</code>은 <code>Overfitting</code> 억제를 위해 많이 사용하는 방법으로 학습 과정에서 <strong>큰 가중치</strong>에 대해서는 그에 상응하는 큰 패널티를 부과하는 방식으로 진행됩니다. 가중치 매개변수의 값이 커서 발생하는 과대적합을 패널티에 의한 <strong>가중치 감소(Weight Decay)</strong> 를 통해 억제하는 것입니다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/118387973-8c2df280-b65c-11eb-9fe4-8cf50f02e3c5.png" alt="image"  />
</p>
<p>이러한 가중치에 대한 패널티를 어떤 방식으로 주는지에 따라 <code>L1 Regularization (Lasso)</code>, <code>L2 Regularization (Ridge)</code> 크게 두 가지의 방법이 존재합니다.</p>
<hr>
<h3 id="l1-regularization-lasso">L1 Regularization (Lasso)<a hidden class="anchor" aria-hidden="true" href="#l1-regularization-lasso">#</a></h3>
<p><code>Regularization</code>은 모델을 학습할 때 단순히 손실 뿐만이 아니라 모델의 <strong>복잡도</strong>를 함께 최소화해 모델이 일반화가 될 수 있도록 합니다. 정규화를 적용한 모델의 학습 최적화 알고리즘은 모델이 데이터에 얼마나 적합한지 측정하는 <strong>손실 항</strong>과 모델 복잡도를 측정하는 <strong>정규화 항</strong>의 함수가 됩니다.</p>
<p>이때 <code>L1 Regularization</code>은 정규화 항을 <code>L1 Norm</code><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 을 기반으로 정의합니다. <code>L1 Norm</code>은 Manhattan distance 로 부르기도 하는데 벡터 a, b 가 있을 때 각 원소들의 차이의 절댓값 총합을 의미합니다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/118389836-d6b46c80-b666-11eb-9b7b-94d59d414da7.png" alt="image"  />
</p>
<p>따라서 L1 정규화 항은 <strong>모든 특성 가중치의 절댓값의 합계</strong>로 정의해 모델의 복잡도를 수치화합니다. 아래 식은 기존 비용 함수에 L1 정규화 항을 더한 것입니다.</p>
<ul>
<li>ƛ(lambda) : Regularization 의 강도를 결정하는 하이퍼파라미터</li>
</ul>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/118389803-9d7bfc80-b666-11eb-8eae-aa8eef0837c6.png" alt="image"  />
</p>
<p>L1 정규화를 적용한 비용 함수는 가중치 값에 대한 미분을 통해 가중치에서 일정 상수를 빼는 식으로 가중치를 업데이트합니다.</p>
<p>최종적으로 모델에서 유용하지 않은 일부 가중치를 0이 되도록 유도할 수 있습니다. 가중치가 정확하게 0일 경우 이것은 모델에서 해당 특성을 삭제하는 것과 같습니다.</p>
<ul>
<li><code>L1 Regularization</code>을 사용하는 Regression 모델을 <code>Lasso Regression</code> 이라고 합니다.</li>
</ul>
<hr>
<h3 id="l2-regularization-ridge">L2 Regularization (Ridge)<a hidden class="anchor" aria-hidden="true" href="#l2-regularization-ridge">#</a></h3>
<p><code>L2 Regularization</code>은 정규화 항을 <code>L2 Norm</code>을 기반으로 정의합니다. <code>L2 Norm</code>은 Euclidean distance 로 부르기도 하는데 벡터 a, b 가 있을 때 두 벡터의 직선 거리를 의미합니다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/118389855-f64b9500-b666-11eb-8959-cca1be2e350f.png" alt="image"  />
</p>
<p>따라서 L2 정규화 항은 <strong>모든 특성 가중치를 제곱한 값의 합계</strong>로 정의해 모델의 복잡도를 수치화합니다. 아래 식은 기존 비용 함수에 L2 정규화 항을 더한 것입니다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/118388303-8f29e280-b65e-11eb-9eae-4865381fb1d3.png" alt="image"  />
</p>
<p>L2 정규화를 적용한 비용 함수는 가중치 값에 대한 미분을 통해 아래와 같은 식으로 가중치를 업데이트합니다. 이때 기존 가중치(θj)에 곱해지는 값은 항상 1 보다 작으므로 최종적으로 <strong>가중치는 줄어드는 방향</strong>으로 조정되고 특정 가중치가 비이상적으로 커지는 것을 막습니다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/118391795-1aac6f00-b671-11eb-82ce-b483b4f701b4.png" alt="image"  />
</p>
<p>📌 <code>ƛ(lambda) 값</code>이 너무 높으면 모델은 단순해지지만 일반화가 아니라 <code>Underfitting(과소적합)</code>해질 위험이 있습니다. 반대로 값이 너무 낮으면 모델은 더 복잡해지고 <code>Overfitting(과대적합)</code>해질 위험이 있습니다. 0으로 설정하는 경우 정규화는 완전히 제거되어 가장 높은 수준의 과적합 위험을 갖게 됩니다. 이것은 <code>L1 Regularization</code> 에서도 마찬가지입니다.</p>
<ul>
<li><code>L2 Regularization</code>을 사용하는 Regression 모델을 <code>Ridge Regression</code> 이라고 합니다.</li>
</ul>
<hr>
<h3 id="l1-regularization--l2-regularization">L1 Regularization 🆚 L2 Regularization<a hidden class="anchor" aria-hidden="true" href="#l1-regularization--l2-regularization">#</a></h3>
<p>L1 정규화와 L2 정규화의 가중치 업데이트에 대한 차이점을 좀 더 살펴보겠습니다.</p>
<p>아래 그림의 초록색 부분은 정규화로 가능한 가중치의 범위를 나타내며 빨간색 부분은 비용 함수로 가운데 점은 전역 최솟값을 만드는 최적의 가중치를 의미합니다. 왼쪽 그림은 L1 정규화 오른쪽 그림은 L2 정규화에 대한 내용입니다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/118397228-ff9c2800-b68d-11eb-8d6c-224cb6354db8.png" alt="image"  />
</p>
<p><code>L2 정규화</code>로 가능한 가중치의 범위는 원형의 모양으로 일반적으로 비용 함수와의 접선이 축에 위치하지 않기 때문에 가중치가 0이 되지는 않습니다.</p>
<p><code>L1 정규화</code>로 가능한 가중치의 범위는 뾰족한 모양으로 비용 함수와의 접선이 축에 위치할 가능성이 높기 때문에 특정 가중치가 0이 될 수 있습니다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/118396447-88b16000-b68a-11eb-80de-a3e18d736cd9.png" alt="image"  />
</p>
<p>정리하면 <code>L2 정규화</code>를 적용할 경우 가중치를 0에 가깝게 만들지만 정확이 0이 되지는 않습니다. 즉, 매번 학습을 하면서 가중치의 x% 만큼 제거한다고 생각할 수 있습니다. 때문에 특성의 가중치를 줄여서 좀 더 <strong>단순한</strong> 모델을 만듦으로써 과대적합을 억제할 수 있습니다. <code>L2 정규화</code>는 단순성을 위한 정규화라고도 합니다.</p>
<p><code>L1 정규화</code>를 적용하면 가중치를 정확히 0으로 만들 수 있기 때문에 특정 가중치를 제거할 수 있습니다. 다시 말해 일종의 <strong>Feature Selection</strong> 을 수행해서 Feature 의 개수를 줄이는 방향으로 과대적합을 억제할 수 있습니다. <code>L1 정규화</code>는 희소성(Sparsity)<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>을 위한 정규화라고도 합니다.</p>
<hr>
<h3 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<ul>
<li><a href="https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization">https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization</a></li>
<li><a href="https://www.boostcourse.org/ai222/lecture/24509">https://www.boostcourse.org/ai222/lecture/24509</a></li>
<li>밑바닥부터 시작하는 딥러닝 1 - 한빛미디어 (2017)</li>
<li><a href="https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036">https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036</a></li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Norm : 벡터의 길이 혹은 크기를 측정하는 방법 <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>0이 많이 있는 자료들을 희소성이 있는 자료라 합니다. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://koolganni.github.io/tags/%EC%98%A4%EB%B2%84%ED%94%BC%ED%8C%85/">오버피팅</a></li>
      <li><a href="https://koolganni.github.io/tags/regularization/">Regularization</a></li>
      <li><a href="https://koolganni.github.io/tags/%EC%9D%BC%EB%B0%98%ED%99%94/">일반화</a></li>
    </ul>
  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://koolganni.github.io/">Ganni Devlog</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        container.appendChild(copybutton);
    });
</script>
</body>

</html>
