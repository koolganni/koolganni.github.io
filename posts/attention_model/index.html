<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Attention is All You Need | Ganni Devlog</title>
<meta name="keywords" content="RNN, Attention, Transformer" />
<meta name="description" content="Attention 과 장기의존성 Self-Attention  Attention 과 장기의존성 RNN 모델은 시퀀스 데이터를 처리할 수 있다는 장점이 있지만 동시에 &lsquo;장기 의존성 (Long-term dependency)&rsquo; 문제 또한 가지고 있다.
장기 의존성 문제 (Long-term dependency)란, 시퀀스 즉 문장이 길어질수록 이전 단어의 정보를 잃어버려 정보의 손실이 발생하는 것을 의미한다. 이러한 문제를 해결하기 위해 과거 정보와 현재 정보를 적절히 조절해 활용하는 LSTM, LSTM 의 간소화 버전 GRU 모델이 나왔다.
하지만 LSTM, GRU 모델로도 해결할 수 없는 문제가 있다.">
<meta name="author" content="">
<link rel="canonical" href="https://koolganni.github.io/posts/attention_model/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.b15591da31b4f827d6dc0b97cf931d21b01610305321ac220bf755cf6aade94e.css" integrity="sha256-sVWR2jG0&#43;CfW3AuXz5MdIbAWEDBTIawiC/dVz2qt6U4=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://koolganni.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://koolganni.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://koolganni.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://koolganni.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://koolganni.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.81.0" />
<meta property="og:title" content="Attention is All You Need" />
<meta property="og:description" content="Attention 과 장기의존성 Self-Attention  Attention 과 장기의존성 RNN 모델은 시퀀스 데이터를 처리할 수 있다는 장점이 있지만 동시에 &lsquo;장기 의존성 (Long-term dependency)&rsquo; 문제 또한 가지고 있다.
장기 의존성 문제 (Long-term dependency)란, 시퀀스 즉 문장이 길어질수록 이전 단어의 정보를 잃어버려 정보의 손실이 발생하는 것을 의미한다. 이러한 문제를 해결하기 위해 과거 정보와 현재 정보를 적절히 조절해 활용하는 LSTM, LSTM 의 간소화 버전 GRU 모델이 나왔다.
하지만 LSTM, GRU 모델로도 해결할 수 없는 문제가 있다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://koolganni.github.io/posts/attention_model/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-04-15T20:19:36&#43;09:00" />
<meta property="article:modified_time" content="2021-04-15T20:19:36&#43;09:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Attention is All You Need"/>
<meta name="twitter:description" content="Attention 과 장기의존성 Self-Attention  Attention 과 장기의존성 RNN 모델은 시퀀스 데이터를 처리할 수 있다는 장점이 있지만 동시에 &lsquo;장기 의존성 (Long-term dependency)&rsquo; 문제 또한 가지고 있다.
장기 의존성 문제 (Long-term dependency)란, 시퀀스 즉 문장이 길어질수록 이전 단어의 정보를 잃어버려 정보의 손실이 발생하는 것을 의미한다. 이러한 문제를 해결하기 위해 과거 정보와 현재 정보를 적절히 조절해 활용하는 LSTM, LSTM 의 간소화 버전 GRU 모델이 나왔다.
하지만 LSTM, GRU 모델로도 해결할 수 없는 문제가 있다."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://koolganni.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Attention is All You Need",
      "item": "https://koolganni.github.io/posts/attention_model/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Attention is All You Need",
  "name": "Attention is All You Need",
  "description": "Attention 과 장기의존성 Self-Attention  Attention 과 장기의존성 RNN 모델은 시퀀스 데이터를 처리할 수 있다는 장점이 있지만 동시에 \u0026lsquo;장기 의존성 (Long-term dependency)\u0026rsquo; 문제 또한 가지고 있다.\n장기 의존성 문제 (Long-term dependency)란, 시퀀스 즉 문장이 길어질수록 이전 단어의 정보를 잃어버려 정보의 손실이 발생하는 것을 의미한다. 이러한 문제를 해결하기 위해 과거 정보와 현재 정보를 적절히 조절해 활용하는 LSTM, LSTM 의 간소화 버전 GRU 모델이 나왔다.\n하지만 LSTM, GRU 모델로도 해결할 수 없는 문제가 있다.",
  "keywords": [
    "RNN", "Attention", "Transformer"
  ],
  "articleBody": " Attention 과 장기의존성 Self-Attention  Attention 과 장기의존성 RNN 모델은 시퀀스 데이터를 처리할 수 있다는 장점이 있지만 동시에 ‘장기 의존성 (Long-term dependency)’ 문제 또한 가지고 있다.\n장기 의존성 문제 (Long-term dependency)란, 시퀀스 즉 문장이 길어질수록 이전 단어의 정보를 잃어버려 정보의 손실이 발생하는 것을 의미한다. 이러한 문제를 해결하기 위해 과거 정보와 현재 정보를 적절히 조절해 활용하는 LSTM, LSTM 의 간소화 버전 GRU 모델이 나왔다.\n하지만 LSTM, GRU 모델로도 해결할 수 없는 문제가 있다. 바로 문장이 길어질수록 모든 단어 정보를 고정 길이의 Hidden State Vector 에 담을 수 없다는 것이다. 즉, 입력 문장이 길어질수록 성능이 떨어지는 문제는 여전히 존재하는 것이다.\n따라서 Attention모델은 Hidden State Vector 를 늘려 이 문제를 해결한다. 문장의 입력 단어마다 Hidden State Vector 를 간직하도록 한다. 입력 단어가 N개라면 N개의 Hidden State Vector 를 갖는 것이다.\n영한 번역을 해야 하는 상황을 가정해보자. 영어 문장이 인코더에 들어와 디코더에 한글 문장을 출력해야 한다고 하자.\n디코더에서는 한글 단어를 생성할 때마다 해당 단어가 인코더에서 넘어온 모든 영어 단어의 Hidden State Vector 와 얼마나 연관이 있는지 Attention 점수를 구한다. 구하는 과정은 다음과 같다.\n 인코더의 각 단어 별 Hidden State Vector 와 디코더의 Hidden State Vector 준비 인코더의 각 Hidden State Vector 와 디코더의 Hidden State Vector 를 내적해서 입력 단어 별 가중치 score 계산 가중치 score 에 대해 softmax 함수 처리 → 각 단어가 디코더에서 생성할 단어와 얼마나 연관이 있는지에 대한 가중치 분포 3번에서 구한 softmax score 와 각 단어의 Hidden State Vector 를 가중합 4번에서 구한 Context Vector 를 기반으로 디코더 단어 출력  이러한 과정을 통해 Attention 모델은 매번 하나의 단어를 출력할 때마다 문장의 모든 단어를 가지고 어떤 인코더 Hidden State Vector 와 연관이 있는지, 즉 어떤 단어에 집중(Attention)해야 하는지 알 수 있다.\n다시 말해, 문장의 모든 정보를 매번 활용함으로써 장기 의존성 문제를 해결한다.\nSelf-Attention …\n References\n  https://medium.datadriveninvestor.com/attention-in-rnns-321fbcd64f05 https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/  ",
  "wordCount" : "285",
  "inLanguage": "en",
  "datePublished": "2021-04-15T20:19:36+09:00",
  "dateModified": "2021-04-15T20:19:36+09:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://koolganni.github.io/posts/attention_model/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ganni Devlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://koolganni.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://koolganni.github.io/" accesskey="h" title="Ganni Devlog (Alt + H)">Ganni Devlog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://koolganni.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://koolganni.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Attention is All You Need
    </h1>
    <div class="post-meta">April 15, 2021
</div>
  </header> 
  <div class="post-content"><ul>
<li><a href="#1">Attention 과 장기의존성</a></li>
<li><a href="#2">Self-Attention</a></li>
</ul>
<h2 id="1">Attention 과 장기의존성<a hidden class="anchor" aria-hidden="true" href="#1">#</a></h2>
<p>RNN 모델은 시퀀스 데이터를 처리할 수 있다는 장점이 있지만 동시에 &lsquo;장기 의존성 (Long-term dependency)&rsquo; 문제 또한 가지고 있다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/114864530-0f74e200-9e2c-11eb-8c77-0a3cbf5e530d.png" alt="image"  />
</p>
<p><!-- raw HTML omitted -->장기 의존성 문제 (Long-term dependency)<!-- raw HTML omitted -->란, 시퀀스 즉 문장이 길어질수록 이전 단어의 정보를 잃어버려 <!-- raw HTML omitted -->정보의 손실<!-- raw HTML omitted -->이 발생하는 것을 의미한다. 이러한 문제를 해결하기 위해 과거 정보와 현재 정보를 적절히 조절해 활용하는 LSTM, LSTM 의 간소화 버전 GRU 모델이 나왔다.</p>
<p>하지만 LSTM, GRU 모델로도 해결할 수 없는 문제가 있다. 바로 문장이 길어질수록 모든 단어 정보를 고정 길이의 Hidden State Vector 에 담을 수 없다는 것이다. 즉, 입력 문장이 길어질수록 성능이 떨어지는 문제는 여전히 존재하는 것이다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/114866646-ca9e7a80-9e2e-11eb-8062-3a0f53eaff07.png" alt="image"  />
</p>
<p>따라서 <!-- raw HTML omitted -->Attention<!-- raw HTML omitted --> 모델은 Hidden State Vector 를 늘려 이 문제를 해결한다. 문장의 입력 단어마다 Hidden State Vector 를 간직하도록 한다. 입력 단어가 N개라면 N개의 Hidden State Vector 를 갖는 것이다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/114866727-e1dd6800-9e2e-11eb-8ea6-8c3c82eefcc4.png" alt="image"  />
</p>
<p>영한 번역을 해야 하는 상황을 가정해보자. 영어 문장이 인코더에 들어와 디코더에 한글 문장을 출력해야 한다고 하자.</p>
<p>디코더에서는 한글 단어를 생성할 때마다 해당 단어가 인코더에서 넘어온 모든 영어 단어의 Hidden State Vector 와 얼마나 연관이 있는지 Attention 점수를 구한다. 구하는 과정은 다음과 같다.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/76609403/114869103-937d9880-9e31-11eb-8862-179af3ee5b03.png" alt="image"  />
</p>
<ol>
<li>인코더의 각 단어 별 Hidden State Vector 와 디코더의 Hidden State Vector 준비</li>
<li>인코더의 각 Hidden State Vector 와 디코더의 Hidden State Vector 를 내적해서 입력 단어 별 가중치 score 계산</li>
<li>가중치 score 에 대해 softmax 함수 처리 → 각 단어가 디코더에서 생성할 단어와 얼마나 연관이 있는지에 대한 가중치 분포</li>
<li>3번에서 구한 softmax score 와 각 단어의 Hidden State Vector 를 가중합</li>
<li>4번에서 구한 Context Vector 를 기반으로 디코더 단어 출력</li>
</ol>
<p>이러한 과정을 통해 Attention 모델은 매번 하나의 단어를 출력할 때마다 문장의 모든 단어를 가지고 어떤 인코더 Hidden State Vector 와 연관이 있는지, 즉 어떤 단어에 <!-- raw HTML omitted -->집중(Attention)<!-- raw HTML omitted -->해야 하는지 알 수 있다.</p>
<p>다시 말해, 문장의 모든 정보를 매번 활용함으로써 장기 의존성 문제를 해결한다.</p>
<!-- raw HTML omitted -->
<h2 id="2">Self-Attention<a hidden class="anchor" aria-hidden="true" href="#2">#</a></h2>
<p>&hellip;</p>
<blockquote>
<p>References</p>
</blockquote>
<ul>
<li><a href="https://medium.datadriveninvestor.com/attention-in-rnns-321fbcd64f05">https://medium.datadriveninvestor.com/attention-in-rnns-321fbcd64f05</a></li>
<li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></li>
</ul>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://koolganni.github.io/tags/rnn/">RNN</a></li>
      <li><a href="https://koolganni.github.io/tags/attention/">Attention</a></li>
      <li><a href="https://koolganni.github.io/tags/transformer/">Transformer</a></li>
    </ul>
  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://koolganni.github.io/">Ganni Devlog</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        container.appendChild(copybutton);
    });
</script>
</body>

</html>
