<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RNN on Ganni Devlog</title>
    <link>https://koolganni.github.io/tags/rnn/</link>
    <description>Recent content in RNN on Ganni Devlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>kr-ko</language>
    <lastBuildDate>Thu, 15 Apr 2021 20:19:36 +0900</lastBuildDate><atom:link href="https://koolganni.github.io/tags/rnn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Attention is All You Need</title>
      <link>https://koolganni.github.io/posts/attention_model/</link>
      <pubDate>Thu, 15 Apr 2021 20:19:36 +0900</pubDate>
      
      <guid>https://koolganni.github.io/posts/attention_model/</guid>
      <description>Attention 과 장기의존성 Self-Attention  Attention 과 장기의존성 RNN 모델은 시퀀스 데이터를 처리할 수 있다는 장점이 있지만 동시에 &amp;lsquo;장기 의존성 (Long-term dependency)&amp;rsquo; 문제 또한 가지고 있다.
장기 의존성 문제 (Long-term dependency)란, 시퀀스 즉 문장이 길어질수록 이전 단어의 정보를 잃어버려 정보의 손실이 발생하는 것을 의미한다. 이러한 문제를 해결하기 위해 과거 정보와 현재 정보를 적절히 조절해 활용하는 LSTM, LSTM 의 간소화 버전 GRU 모델이 나왔다.
하지만 LSTM, GRU 모델로도 해결할 수 없는 문제가 있다.</description>
    </item>
    
  </channel>
</rss>
