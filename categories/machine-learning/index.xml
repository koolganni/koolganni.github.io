<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Ganni Devlog</title>
    <link>https://koolganni.github.io/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Ganni Devlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>kr-ko</language>
    <lastBuildDate>Sun, 11 Apr 2021 19:22:21 +0900</lastBuildDate><atom:link href="https://koolganni.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>경사 하강법 (Gradient Descent)</title>
      <link>https://koolganni.github.io/posts/machine-learning/gradient-descent/</link>
      <pubDate>Sun, 11 Apr 2021 19:22:21 +0900</pubDate>
      
      <guid>https://koolganni.github.io/posts/machine-learning/gradient-descent/</guid>
      <description>머신 러닝 그리고 딥러닝 모델 학습의 목표는 더 정확한 예측을 위해 최적의 가중치와 편향을 찾는 것입니다. 이를 위한 방법으로 주로 사용되는 경사 하강법에 대해 알아보겠습니다.
 비용 함수 (Cost Function) X 축은 입력값이고 Y 축은 목푯값입니다. 이때 X 를 입력하면 Y 를 예측하여 출력하는 모델을 만든다고 가정해봅시다. 머신 러닝에서 𝑊 는 가중치 벡터를 의미하고 𝑏 는 편향을 나타냅니다.
위 그림에서는 3가지 선들 중 파란색 선이 모든 데이터 세트를 정확하게 예측한 것으로 보입니다.</description>
    </item>
    
    <item>
      <title>의사결정 나무 (Decision Tree)</title>
      <link>https://koolganni.github.io/posts/machine-learning/decision-tree/</link>
      <pubDate>Sat, 20 Feb 2021 18:51:50 +0900</pubDate>
      
      <guid>https://koolganni.github.io/posts/machine-learning/decision-tree/</guid>
      <description>LightGBM XGBoost와 같은 Gradient Boosting Decision Tree (GBDT) 기반 앙상블 모델은 Kaggle Competition 상위권 팀에서 많이 사용하는 모델로 유명합니다. 이번 글에서는 Decision Tree 분류 모델에 대해 알아보겠습니다.
 Decision Tree Decision Tree (의사결정 나무)는 데이터에 내재되어 있는 패턴을 예측 가능한 분류 규칙들의 조합으로 나타내어 목표 변수에 대한 분류(Classification) 또는 예측(Regression)을 수행하는 기법입니다. 질문을 던지면서 찾고자 하는 대상을 좁혀나가는 일련의 필터링 과정 혹은 스무고개 놀이와 비슷한 개념으로 아래 그림과 같이 트리의 형태를 가지고 있습니다.</description>
    </item>
    
    <item>
      <title>로지스틱 회귀 (Logistic Regression)</title>
      <link>https://koolganni.github.io/posts/machine-learning/logistic-regression/</link>
      <pubDate>Sat, 13 Feb 2021 19:44:41 +0900</pubDate>
      
      <guid>https://koolganni.github.io/posts/machine-learning/logistic-regression/</guid>
      <description>이진 분류 (Binary Classification) 문제에서 좋은 성능을 보이며 동시에 딥러닝에서 중요한 개념을 포함하고 있는 로지스틱 회귀(분류)에 대해 알아보겠습니다.
 Binary Classification 스팸 메일과 스팸이 아닌 메일, 긍정 리뷰와 부정 리뷰, 악성 종양과 양성 종양 등 두 가지 선택지 중에서 하나를 예측하는 것을 우리는 이진 분류(Binary Classification)라고 합니다.
이진 분류 문제를 해결하기 위해서는 어떤 모델을 사용해야 할까요? 아래와 같이 종양의 크기에 따라 악성(1)인지 양성(0)인지 분류하는 선형 회귀 모델을 만든다고 가정해봅시다.</description>
    </item>
    
    <item>
      <title>L1 과 L2 정규화 (Regularization)</title>
      <link>https://koolganni.github.io/posts/machine-learning/l1-l2-regularization/</link>
      <pubDate>Sat, 06 Feb 2021 20:21:54 +0900</pubDate>
      
      <guid>https://koolganni.github.io/posts/machine-learning/l1-l2-regularization/</guid>
      <description>모델의 성능을 높이는 데에는 여러 방법이 있습니다. Overfitting(과대적합)을 억제하는 방법 중 L1 정규화 및 L2 정규화 기술에 대해 알아보겠습니다.
 Overfitting 아래 그림의 맨 오른쪽은 모델이 주어진 데이터 세트에 대해 지나치게 많이 학습을 한 Overfitting(과대적합)의 모습을 보여줍니다. 이는 모델이 학습 데이터를 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있습니다.
학습 데이터에 모델이 Overfitting 되는 현상은 모델의 성능을 떨어트리는 주요 이슈입니다. 훈련 데이터에 대한 정확도는 높을지라도 새로운 데이터, 즉 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않습니다.</description>
    </item>
    
  </channel>
</rss>
